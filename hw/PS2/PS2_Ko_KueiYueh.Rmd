---
title: "BIOS707 | Problem set2"
author: Kuei-Yueh (Clint) Ko
output: html_notebook
---

# background

This document is the **problem set 2** of the course BIOS707. The code chunk below includes the library needed and the setting of working directory

Set environment
```{r, message=FALSE, warning = FALSE}
### import library
# tools
library(tidyverse)
library(scales)
library(ICC)
library(zoo)

# plotting
library(gridExtra)
library(RColorBrewer)
library(corrplot)

### set directory (remember to set your working directory)
workdir <- "/home/clint/GitRepo/Duke_BIOS707_ML/hw/PS2" 
#workdir <- "/mnt/c/Users/clint/GitRepo/Duke_BIOS707_ML"
setwd(workdir)
```


# Simulation

## 1. Missing Data
We will explore missing data mechanisms and basic imputation strategies. Follow these steps showing your code. Don't forget to set your seeds.

### (a) set your number of people 'n' to 1000
```{r}
N <- 1000
```

### (b, c, d) Simulation and calcualte the mean
simulate $Z \sim N(0, 1)$ and $X \sim N(Z, 1)$. Calculate the mean of complete X 

```{r}
### initialization
set.seed(0)

### simulate a value of z
z <- rnorm(N, mean = 0, sd = 1)

### simulate a complete vector of x
# both options result in the same results
x_c <- rnorm(N, mean = z, sd = 1)                          # option 01
#x_c <- sapply(z, function(x){rnorm(1, mean = x, sd = 1)}) # option 02

cat("Mean(x_c): ", mean(x_c))
```


### (e) Make 10% of the X  values missing. Calculate  the mean of this new X. Comment on any differences or similarities 

Set the observed X where 10% values are missing at random.  Compare observed X and complete X.
```{r}
### set 90% of x missing (observe)
x_o <- x_c
x_o[sample(1:N, size = 100)] <- NA


### quick view of the simulated data
cat("", 
    "Mean(x_c): ", mean(x_c),   "\n",
    "SD(x_c):   ", sd(x_c),     "\n\n",
    
    "Mean(x_o): ", mean(x_o, na.rm = TRUE), "\n",
    "SD(x_o):   ", sd(x_o,   na.rm = TRUE))
```


(i) **What is the missing data mechanism?**
    - Missing completely at random (MCAR)

    
(ii) **Do we expect $E[X^c] = E[X^o]$**
    - yes, below are two arguments that support the equation.

This equation can be support by the following results: First, from 95% CIs of the empirical (estimated) means, we know that there is no evidence to reject that the null hypothesis that the mean between complete and observed are equal. We could also reach the same conclusion using simulation. By repeat the same procedure of producing 100 z and x values, the results appear that the observed mean and complete mean distribute around the 45 degree line. 
    
```{r, echo = FALSE, fig.height=4, fig.width=4}
### compute 95% CI
tmp_mu <- c(mean(x_c), mean(x_o, na.rm = TRUE))
tmp_sd <- c(  sd(x_c),   sd(x_o, na.rm = TRUE))

###
df <- data.frame(
    x    = c("X_c (complete)", "X_o (observed)"),
    Mean = tmp_mu,
    L    = tmp_mu - qnorm(0.975) * tmp_sd,
    U    = tmp_mu + qnorm(0.975) * tmp_sd)#

### 
gp1 <- ggplot(df, aes(x = x, y = Mean)) +
    geom_point(size = 3) +
    geom_errorbar(aes(ymax = U, ymin = L)) +
    ggtitle("95% CI of Mean") + 
    theme(
        axis.title.x = element_blank(),
        axis.title.y  = element_text(size = 12),
        axis.text.x  = element_text(size  = 12))
```

```{r, echo = FALSE, fig.height=5, fig.width=5}
### simulate complete and observed data
### function to perform simulation
missing_data_simulation <- function(){
    ### initialization
    N <- 1000
    
    ### simulate a value of z
    z <- rnorm(N, mean = 0, sd = 1)
    
    ### simulate a complete vector of x
    x_c <- rnorm(N, mean = z, sd = 1)

    ### set 90% of x missing (observe)
    x_o <- x_c
    x_o[sample(1:N, size = 100)] <- NA
    
    res <- c(mean(x_c, na.rm = TRUE), mean(x_o, na.rm = TRUE))
    names(res) <- c("mean (complete)", "mean (observed)")
    return(res)
} # end func

### simulate multiple times
set.seed(0)
df <- replicate(5000, missing_data_simulation())
df <- data.frame(t(df))
colnames(df) <- c("mu_Xc", "mu_Xo")
    
### show the result
gp2 <- ggplot(df, aes(x = mu_Xc, y = mu_Xo)) +
    geom_point(size = 1, alpha = 0.3) +
    geom_abline(intercept = 0, slope = 1, color = "red") +
    labs(title = "Simulation of complete and observed empirical mean",
         x     = "mean (complete)", 
         y     = "mean (observed)")
```

```{r, echo = FALSE}
grid.arrange(gp1, gp2, nrow = 1)
```


(iii) Using the mean of the observed X values, impute in the missing X-values

```{r}
x_impute <- x_o
x_impute[is.na(x_impute)] <- mean(x_o, na.rm = TRUE)
```


(iv) Does this change our estimate for the mean of X? Do we expect it to?
    - No, the mean is the same.
    - Yes, we expect that the mean are the same before and after the imputation.

```{r}
cat("", 
    "Mean(x_c):      ", mean(x_c),               "\n",
    "Mean(x_o):      ", mean(x_o, na.rm = TRUE), "\n",
    "Mean(x_impute): ", mean(x_impute, na.rm = TRUE))
```

compare the complete and observed $\rightarrow$ I do not reject the null
```{r}
res_test <- t.test(x_c, x_o)
res_test$p.value
```

compare the complete and imputed $\rightarrow$ I do not reject the null
```{r}
res_test <- t.test(x_c, x_impute)
res_test$p.value
```

### (f) Return to your complete data. Among Z > 0, make 10% of the X values missing. Calculate the mean of this new X. Comment on any differences or similarities

Among Z > 0, make 10% of the X values missing.
```{r}
### set seed
set.seed(0)

### set missing values
x_o <- x_c
idx <- which(z > 0)
x_o[sample(idx, size = floor(length(idx) * 0.1))] <- NA
```

(i)   **What is the missing data mechanism?**
    - Missing at Random (MAR)


(ii)  **Do we expect $E[X^c] = E[X^o]$**
    - No, but it is conditionally the same

```{r}
cat("", 
    "Mean(x_c):      ", mean(x_c),               "\n",
    "Mean(x_o):      ", mean(x_o, na.rm = TRUE), "\n",
    "Mean(x_c|Z > 0):", mean(x_c[z > 0], na.rm = TRUE), "\n",
    "Mean(x_o|Z > 0):", mean(x_o[z > 0], na.rm = TRUE), "\n",
    "Mean(x_c|Z < 0):", mean(x_c[z < 0], na.rm = TRUE), "\n",
    "Mean(x_o|Z < 0):", mean(x_o[z < 0], na.rm = TRUE), "\n")
```

Support my answer using t-test  
```{r}
res_test1 <- t.test(x_c, x_o)
res_test2 <- t.test(x_c[z > 0], x_o[z > 0])
res_test3 <- t.test(x_c[z < 0], x_o[z < 0])

cat("Compare E[X_complete] vs E[X_observed]: \n\t", 
    "p-value:", res_test1$p.value, "\n",
    "Compare E[X_complete | Z > 0] vs E[X_observed | Z > 0]: \n\t", 
    "p-value:", res_test2$p.value, "\n", 
    "Compare E[X_complete | Z < 0] vs E[X_complete | Z < 0]: \n\t", 
    "p-value:", res_test3$p.value)
```

The p value of testing $E[X^c] = E[X^o]$ is lower when comparing $E[X^c | Z > 0] = E[X^o | Z > 0]$ and comparing $E[X^c | Z < 0] = E[X^o | Z < 0]$. 


(iii) **Using the mean of the observed X values, impute in the missing X-values. Does this improve our estimate of the mean of X?**
    - No

observe the mean of complete, observed, and imputed x values.
```{r}
x_impute <- x_o
x_impute[is.na(x_impute)] <- mean(x_o, na.rm = TRUE)

cat("", 
    "Mean(x_c):      ", mean(x_c),               "\n",
    "Mean(x_o):      ", mean(x_o, na.rm = TRUE), "\n",
    "Mean(x_impute): ", mean(x_impute, na.rm = TRUE))
```

Support my answer using t-test: At 95% level, we do not reject the null for both test $E[X^c] = E[X^o]$ and $E[X^c] = E[X^{impute}]$ and both tests reported similar p-values.
```{r}
res_test1 <- t.test(x_c, x_o)
res_test2 <- t.test(x_c, x_impute)

cat("Compare E[X_complete] vs E[X_observed]: \n\t", 
    "p-value:", res_test1$p.value, "\n",
    "Compare E[X_complete] vs E[X_imputed]: \n\t", 
    "p-value:", res_test2$p.value)
```



(iv)  **We will perform conditional imputation:**

    - A. Using the observed X, Z pairs fit a linear regression model, regressing X onto Z. This can be thought of as a simple predictive model for X based on Z.
    
```{r}
fit <- lm(x_o ~ z)
print(fit)
```

    - B. Using this fit, generate predictive values for X, based on Z. hint: use the predict() function for lm.

```{r}
x_pred <- predict(fit, newdata = list(z))
```
    

    - C. Calculate new imputed mean of X. How does this compare to the true mean of X.
        - The predicted mean becomes similar to the true mean of X. The conclusion can be supported by the t-test as well.
```{r}
cat("", 
    "Mean(x_c):      ", mean(x_c),                  "\n",
    "Mean(x_o):      ", mean(x_o,    na.rm = TRUE), "\n",
    "Mean(x_pred):   ", mean(x_pred, na.rm = TRUE))
```

```{r}
res_test1 <- t.test(x_c, x_o)
res_test2 <- t.test(x_c, x_pred)

cat("Compare E[X_complete] vs E[X_observed]: \n\t", 
    "p-value:", res_test1$p.value, "\n",
    "Compare E[X_complete] vs E[X_predicted]: \n\t", 
    "p-value:", res_test2$p.value)
```

### (g) What would happen if in (f) we had set the missingness degree to be 100%? Would we be able to recover the true mean value?

remove all z > 0
```{r}
### set missing values
x_o <- x_c
idx <- which(z > 0)
x_o[idx] <- NA
```

impute the missing value from the regression model
```{r}
fit <- lm(x_o ~ z)
x_pred <- predict(fit, newdata = list(z))
```

The results of the t-test can be used as an evidence to show that the value could be recovered even we had set the missingness degree to be 100% (remove 100% of value where z > 0).

Before imputation, the means are different, while after imputation, we do not reject that the means are the same.
```{r}
res_test1 <- t.test(x_c, x_o)
res_test2 <- t.test(x_c, x_pred)

cat("Compare E[X_complete] vs E[X_observed]: \n\t", 
    "p-value:", res_test1$p.value, "\n",
    "Compare E[X_complete] vs E[X_predicted]: \n\t", 
    "p-value:", res_test2$p.value)
```


## 2. Intraclass Correlation Coefficient (ICC) THe ICC is a statistic that measures the ratio of within group to between group variance. It assess how tightly correlated

The ICC is a statistic that measures the ratio of within group to between group variance. It assess how tightly correlated a measure is based on some grouping variable. It is particularly useful in longitudinal studies where patients are measured repeatedly overtime. For example we may measure blood pressure on a patients multiple times. The ICC will assess whether there is more variation within people than between people. The ICC varies between 0 - 1. A value of 1 indicates that there is more between person variation while a value of 0 indicates more within person variation (see Figure 1)

There are multiple ways of defining the ICC (see http://en.wikipedia.org/wiki/Intraclass_correlation). We will use the random effects definition:

$$Y_{ij} = \mu + \alpha_j + \epsilon_{ij}$$

Where $Y_{ij}$ is the outcome for person $i$ in group $j$; $\alpha_j$ is a random effect shared by all people in up $j$. We assume: 

- $\epsilon \sim N(0, \sigma^2_{\epsilon})$
- $\alpha \sim N(0, \sigma^2_{\alpha})$
- $\alpha_j \perp \epsilon_{ij}$. 

Then:

$$\text{ICC} = \frac{\sigma^2_{\alpha}}{\sigma^2_{\alpha} + \sigma^2_{\epsilon}}$$

(a) **Using the random effects definition of an ICC, write a function that simulates data with a user specified ICC. The function should take as arguments a way to specify the desired ICC, a total sample size and the number of groups. You can allow the groups be of the same size. Show your function code.**

$$\text{ICC} = \frac{\sigma^2_{\alpha}}{\sigma^2_{\alpha} + \sigma^2_{\epsilon}}$$

$$\frac{1}{\text{ICC}} = \frac{\sigma^2_{\alpha} + \sigma^2_{\epsilon}}{\sigma^2_{\alpha}} = 1 + \frac{\sigma^2_{\epsilon}}{\sigma^2_{\alpha}}$$

$$\frac{1}{\text{ICC}} - 1 = \frac{\sigma^2_{\epsilon}}{\sigma^2_{\alpha}}$$

$$\sigma^2_{\epsilon} = \sigma^2_{\alpha} \Big( \frac{1}{\text{ICC}} - 1 \Big) $$

```{r}
simulate_icc <- function(icc, grp_size = 10, n_grp = 5, mu_tot = 0, sig2_a = 1){
    # function to simuate the data from specific ICC
    # =================================
    
    ### calculate sig2_e from the ICC and sig2_a
    sig2_e <- sig2_a * (1 / icc - 1)
    
    ### simulate alpha by sig2_a
    alpha  <- rnorm(n_grp, mean = 0, sd = sig2_a^0.5)
    
    ### simulate error for each group by sig2_e
    df <- sapply(mu_tot + alpha, function(x){
        eps <- rnorm(grp_size, mean = 0, sd = sig2_e^0.5)
        return(x + eps)
    })

    ### set column names
    df <- data.frame(df)
    colnames(df) <- paste("Grp", 1:n_grp)

    ### gather and return
    df <- df %>% gather(Group, Value)
    return(df)
} # end func
```


(b) **Simulate data that has a theoretical ICC of 0.1 and 0.9 across k = 10 groups using a total sample size of n = 100. Use the R function ICCest() in the ICC package to estimate your empirical ICC and 95% confidence interval.**

```{r, fig.width = 10, fig.height = 4}
###
K          <- 10
N          <- 100
ICC1       <- 0.1
ICC2       <- 0.9
SIG2_ALPHA <- 1

###
df1 <- simulate_icc(ICC1, n_grp = K, grp_size = floor(N / K), sig2_a = SIG2_ALPHA)
df1$ICC <- paste("ICC", "=", ICC1)

###
df2 <- simulate_icc(ICC2, n_grp = K, grp_size = floor(N / K), sig2_a = SIG2_ALPHA)
df2$ICC <- paste("ICC", "=", ICC2)

###
df  <- bind_rows(df1, df2)
df$Group <- factor(df$Group, levels = paste("Grp", 1:K))

###
gp <- ggplot(df, aes(x = Group, y = Value, color = Group)) + 
    geom_jitter(width = 0.2) +
    ggtitle("Simulating random effects with different ICC") +
    theme(
        axis.title.y = element_text(size = 15),
        axis.title.x = element_text(size = 15),
        axis.text.x  = element_text(size = 12, angle = 90, vjust = 0.5),
        strip.text.x = element_text(size = 20, color = "grey30")) +
    facet_wrap(~ICC)
print(gp)
```


Use the R function ICCest() in the ICC package to estimate the empirical ICC and 95% confidence interval
```{r, echo = FALSE, fig.width = 3, fig.height = 5}
### calculate ICC for two ICC = 0.1 and ICC = 0.9
res_icc01 <- ICCest(x = Group, y = Value, data = df1)
res_icc02 <- ICCest(x = Group, y = Value, data = df2)

### arrange results
df <- data.frame(
    x = c("ICC = 0.1", "ICC = 0.9"),
    y = c(res_icc01$ICC,     res_icc02$ICC),
    L = c(res_icc01$LowerCI, res_icc02$LowerCI),
    U = c(res_icc01$UpperCI, res_icc02$UpperCI))

### plot 95% CI
gp <- ggplot(df, aes(x = x, y = y)) +
    geom_point(size = 2) +
    geom_errorbar(aes(ymax = U, ymin = L)) +
    ylim(-0.1, 1) + 
    ggtitle("95% CI of ICC") + 
    theme(
        axis.title.x = element_blank(),
        axis.title.y  = element_text(size = 12),
        axis.text.x  = element_text(size  = 12))
print(gp)
```


(c) **Repeat using a total sample size = of n = 1000**

```{r}
### initialization
K          <- 10
N          <- 1000
ICC1       <- 0.1
ICC2       <- 0.9
SIG2_ALPHA <- 1

### simulation of ICC1 = 0.1
df1 <- simulate_icc(ICC1, n_grp = K, grp_size = floor(N / K), sig2_a = SIG2_ALPHA)
df1$ICC <- paste("ICC", "=", ICC1)

### simulation of ICC2 = 0.9
df2 <- simulate_icc(ICC2, n_grp = K, grp_size = floor(N / K), sig2_a = SIG2_ALPHA)
df2$ICC <- paste("ICC", "=", ICC2)

### combine the results
df  <- bind_rows(df1, df2)
df$Group <- factor(df$Group, levels = paste("Grp", 1:K))

### plot the results
gp <- ggplot(df, aes(x = Group, y = Value, color = Group)) + 
    geom_jitter(width = 0.2) +
    ggtitle("Simulating random effects with different ICC") +
    theme(
        axis.title.y = element_text(size = 15),
        axis.title.x = element_text(size = 15),
        axis.text.x  = element_text(size = 12, angle = 90, vjust = 0.5),
        strip.text.x = element_text(size = 20, color = "grey30")) +
    facet_wrap(~ICC)
print(gp)
```

plot the 95% CI
```{r, echo = FALSE,  fig.width = 3, fig.height = 5}
### calculate ICC for two ICC = 0.1 and ICC = 0.9
res_icc01 <- ICCest(x = Group, y = Value, data = df1)
res_icc02 <- ICCest(x = Group, y = Value, data = df2)

### arrange the results
df <- data.frame(
    x = c("ICC = 0.1", "ICC = 0.9"),
    y = c(res_icc01$ICC,     res_icc02$ICC),
    L = c(res_icc01$LowerCI, res_icc02$LowerCI),
    U = c(res_icc01$UpperCI, res_icc02$UpperCI))

### plot the results
gp <- ggplot(df, aes(x = x, y = y)) +
    geom_point(size = 2) +
    geom_errorbar(aes(ymax = U, ymin = L)) +
    ylim(-0.1, 1) + 
    ggtitle("95% CI of ICC") + 
    theme(
        axis.title.x = element_blank(),
        axis.title.y  = element_text(size = 12),
        axis.text.x  = element_text(size  = 12))
print(gp)
```


(d) **Keep the sample size at n = 1000 but increase the number of groups to k = 100**
```{r}
### initialization
K          <- 100
N          <- 1000
ICC1       <- 0.1
ICC2       <- 0.9
SIG2_ALPHA <- 10

### simulation of ICC1 = 0.1 
df1 <- simulate_icc(ICC1, n_grp = K, grp_size = floor(N / K), sig2_a = SIG2_ALPHA)
df1$ICC <- paste("ICC", "=", ICC1)

### simulation of ICC1 = 0.9
df2 <- simulate_icc(ICC2, n_grp = K, grp_size = floor(N / K), sig2_a = SIG2_ALPHA)
df2$ICC <- paste("ICC", "=", ICC2)

###
df  <- bind_rows(df1, df2)
df$Group <- factor(df$Group, levels = paste("Grp", 1:K))
```

plot the 95% CI
```{r, echo = FALSE, fig.width = 3, fig.height = 5}
### calculate ICC for two ICC = 0.1 and ICC = 0.9
res_icc01 <- ICCest(x = Group, y = Value, data = df1)
res_icc02 <- ICCest(x = Group, y = Value, data = df2)

### arrange the results
df <- data.frame(
    x = c("ICC = 0.1", "ICC = 0.9"),
    y = c(res_icc01$ICC,     res_icc02$ICC),
    L = c(res_icc01$LowerCI, res_icc02$LowerCI),
    U = c(res_icc01$UpperCI, res_icc02$UpperCI))

### plot 95% CI
gp <- ggplot(df, aes(x = x, y = y)) +
    geom_point(size = 2) +
    geom_errorbar(aes(ymax = U, ymin = L)) +
    ylim(0, 1) + 
    ggtitle("95% CI of ICC") + 
    theme(
        axis.title.x = element_blank(),
        axis.title.y = element_text(size = 12),
        axis.text.x  = element_text(size = 12))
    
print(gp)
```


(e) Comment on differences in the precision of your estimate based on k and n. Graphically display your results. Feel free to do more testing to justify your conclusions.

**my answer**
As the number of groups increase, the standard error of ICC decreases.


# Working With Data

Download the Diabetes data set from the UCI Machine Learning Repository:
https://archive.ics.uci.edu/ml/datasets/Diabetes

```{r}
datadir <- "Diabetes-Data"
system("head -6 Diabetes-Data/data-01", intern = TRUE)
```

**Format of the data**  

1. Date in MM-DD-YYYY format  
2. Time in XX:YY format  
3. Code  
4. Value  

**Data code**  

- 33 = Regular insulin dose  
- 34 = NPH insulin dose  
- 35 = UltraLente insulin dose  
- 48 = Unspecified blood glucose measurement  
- 57 = Unspecified blood glucose measurement  
- 58 = Pre-breakfast blood glucose measurement  
- 59 = Post-breakfast blood glucose measurement  
- 60 = Pre-lunch blood glucose measurement  
- 61 = Post-lunch blood glucose measurement  
- 62 = Pre-supper blood glucose measurement  
- 63 = Post-supper blood glucose measurement 
- 64 = Pre-snack blood glucose measurement 
- 65 = Hypoglycemic symptoms  
- 66 = Typical meal ingestion  
- 67 = More-than-usual meal ingestion  
- 68 = Less-than-usual meal ingestion  
- 69 = Typical exercise activity  
- 70 = More-than-usual exercise activity  
- 71 = Less-than-usual exercise activity  
- 72 = Unspecified special event  

**Choose one person**
```{r}
### choose the first person
filename <- "data-01"
```

## Q1. Summarizing Data

import Diabetes data
```{r}
### specify the type of each columns
coltypes <- list(
    col_date(format = "%m-%d-%Y"),
    col_time(format = "%H:%M"),
    col_integer(),
    col_integer())

### import data
dat_raw <- read_delim(
    file.path(datadir, filename), 
    delim = "\t", 
    col_names = FALSE,
    col_types = coltypes) %>%
    `colnames<-`(c("date", "time", "code", "value"))

### observe
head(dat_raw)
```

(a) Select the 3 pre-meal blood glucose measurements (codes 58, 60, 62). Create a table reporting any relevant summary statistics. Provide any interpretation.

- 58 = Pre-breakfast blood glucose measurement  
- 60 = Pre-lunch blood glucose measurement  
- 62 = Pre-supper blood glucose measurement  
```{r}
### initialization (set code_name)
tmp <-  c("Pre-breakfast\nblood glucose measurement",
          "Pre-lunch\nblood glucose measurement",  
          "Pre-supper\nblood glucose measurement")

tmp <-  data.frame(
            code      = c(58, 60, 62), 
            code_name = tmp)

### get data with code 58, 60, 62 (pre-meal blood glucose measurements)
df <- dat_raw
df <- df %>% 
    filter(code %in% c(58, 60, 62)) %>%
    inner_join(., tmp, by = "code")

### specify factor levels of code_name
df$code_name <- factor(df$code_name, levels = tmp$code_name)

### add datetime
df <- df %>% mutate(datetime = paste(date, time))
df$datetime <- parse_datetime(
    df$datetime, 
    format = "%Y-%m-%d %H:%M:%S")

dat_datetime <- df
```

plot the distribution
```{r}
gp <- ggplot(dat_datetime, aes(x = code_name, y = value, group = code_name))
gp <- gp +
    geom_boxplot() + 
    geom_jitter(width = 0.2, alpha = 0.5) +
    labs(x = "", y = "Value", 
         title = "Pre-meal Blood Glucose (BG) Measurements") +
    theme(axis.title.y = element_text(size = 12),
          axis.text.x  = element_text(size = 10))
print(gp)
```

It seems that the blood glucose measurement is a little bit lower during the pre-lunch comparing to pre-breakfast and pre-supper.


Show the ICC and its 95% confidence interval.
```{r, fig.width = 3, fig.height = 4}
res_icc <- ICCest(x = code_name, y = value, data = dat_datetime)
df <- data.frame(
    x = "ICC of Three Pre-meal\nBG Measurements",
    y = res_icc$ICC,
    L = res_icc$LowerCI,
    U = res_icc$UpperCI)

### 
gp <- ggplot(df, aes(x = x, y = y)) +
    geom_point(size = 2) +
    geom_errorbar(aes(ymax = U, ymin = L)) +
    ggtitle(paste("ICC =", round(res_icc$ICC, 2))) + 
    ylim(-0.1, 1.0) +
    theme(
        axis.title.x = element_blank(),
        axis.title.y = element_text(size = 12),
        axis.text.x  = element_text(size = 12))
    
print(gp)
```

There is not much variation between groups


## Q2. Working with Dates


(a) Choose an appropriate plot to display the 3 blood glucose measurements over time

display the time series data
```{r}
gp <- ggplot(dat_datetime, aes(x = datetime, y = value, color = code_name))
gp <- gp +
    geom_point() +
    geom_smooth()
print(gp)
```

(b) Using the 3 meal measurements from above, align the data on a daily basis, i.e. transform the data to a wider format where each line represents a day and has a measurement per-person. Comment on any generated missing data. Show your code.
```{r}
df <- dat_datetime
df <- df %>% group_by(date, code_name) %>% summarize(Mean = mean(value))
df <- df %>% spread(code_name, Mean)
#df$date <- factor(df$date, levels = df$date)

dat_daily <- df
head(dat_daily)
```

show the places of missing value
```{r}
df <- dat_daily
df <- df %>% gather(measurement, value, -date) #%>% mutate(value = is.na(value))

gp <- ggplot(df, aes(x = measurement, y = date, fill = value)) + geom_tile()
print(gp)
```

it turns out that most missing value occurred at pre-lunch blood glucose measurement.

(c) Correlation plots are a way to visualize multivariate relationships. Use the corrplot package to make a correlation plot. What happens if you ignore the missing data?


first plot the correlation by remove missing data.
```{r}
### calculate correlations
mat_cor <- dat_daily %>% as.data.frame %>% dplyr::select(-date) %>% na.omit %>% as.matrix

### colors
colors = brewer.pal(name = "RdYlBu", n = 8)
colors = rev(colors)
colors = colorRampPalette(colors)(100)

### plot the corr matrix
M <- cor(mat_cor)
corrplot.mixed(M, lower.col = colors, upper.col = colors)
```

If ignoring the NA values, the correlation could not be calculated and error occurs
```{r}
### calculate correlations; not removing NA values
mat_cor <- dat_daily %>% as.data.frame %>% dplyr::select(-date) %>% as.matrix

### colors
colors = brewer.pal(name = "RdYlBu", n = 8)
colors = rev(colors)
colors = colorRampPalette(colors)(100)

### plot the corr matrix
M <- cor(mat_cor)
corrplot.mixed(M, lower.col = colors, upper.col = colors)
```



(d) One version of single imputation appropriate for longitudinal data is called "hot-deck" imputation or “last observed carried forward (locf)”. Here the data are ordered (typically by time) and the last observed value is imputed into any time slots without an observed value. Using the data above perform two versions of LOCF and recalculate your correlation plots.

    - i. Carry the last observation forward separately for each of the 3 categories

Note that the order of date at the y axis is from bottom to top.    
```{r}
### locf by column
df <- dat_daily %>% na.locf(., na.rm = FALSE) 

### arrange dataframe from wide to long formate
df <- df %>% gather(measurement, value, -date)

### plot the results
gp <- ggplot(df, aes(x = measurement, y = date, fill = value)) + geom_tile()
print(gp)
```



recalculate correlation matrix
```{r}
### calculate correlations
mat_cor <- df %>% as.data.frame %>% spread(measurement, value) %>% dplyr::select(-date) %>% na.omit %>% as.matrix

### colors
colors = brewer.pal(name = "RdYlBu", n = 8)
colors = rev(colors)
colors = colorRampPalette(colors)(100)

### plot the corr matrix
M <- cor(mat_cor)
corrplot.mixed(M, lower.col = colors, upper.col = colors)
```


    - ii. Carry the last observation forward from the time of day (i.e. impute glucose before lunch using glucose before breakfast). 


```{r}
### locf by row
df <- dat_daily %>% t %>% na.locf(., na.rm = FALSE) %>% t %>% as.data.frame
df$date <- parse_datetime(
    df$date, 
    format = "%Y-%m-%d")

### arrange dataframe from wide to long formate
df <- df %>% 
    gather(measurement, value, -date) %>% 
    mutate(value = as.numeric(value))

### plot the results
gp <- ggplot(df, aes(x = measurement, y = date, fill = value)) + geom_tile()
print(gp)
```
    
recalculate correlation matrix
```{r}
### calculate correlations
mat_cor <- df %>% as.data.frame %>% spread(measurement, value) %>% dplyr::select(-date) %>% na.omit %>% as.matrix

### colors
colors = brewer.pal(name = "RdYlBu", n = 8)
colors = rev(colors)
colors = colorRampPalette(colors)(100)

### plot the corr matrix
M <- cor(mat_cor)
corrplot.mixed(M, lower.col = colors, upper.col = colors)
```

Note: The zoo package has the function na.locf() Comment on implications of each approach. Which approach do you think is better?

**Comparing the recalculated correlation matrix**
The first method of locf provide similar results to the one that was originally calculated, while the second one changed the correlation dramatically. The correlation of pre-breakfast and pre-lunch measurement increase at least two times after imputation using the second method.

**Which approach do you think is better**
I think the first one "Carry the last observation forward separately for each of the 3 categories" because the value of pre-meal blood glucose may have different patterns for breakfast, lunch and supper. Also, for the patient I analyzed, the coorelation matrix changes dramtically if using the second method because the second method increase the correlation of variables.

# Q3. Smoothing 


(a) Fit a lowess smooth over time for each of the three measurements separately. Use the R function lowess(). Set the bandwidth f to 0.1. Plot the results
```{r}
gp <- ggplot(dat_datetime, aes(x = datetime, y = value, color = code_name))
gp <- gp +
    geom_point() +
    geom_smooth(method = 'loess', span = 0.1) +
    theme(legend.position="bottom")

print(gp)
```

(b) Set the bandwidth f to 0.9. Plot the results

```{r}
gp <- ggplot(dat_datetime, aes(x = datetime, y = value, color = code_name))
gp <- gp +
    geom_point() +
    geom_smooth(method = 'loess', span = 0.9) +
    theme(legend.position="bottom")

print(gp)
```

(c) What conclusion does each bandwidth provide

The smaller bandwidth provide short time pattern of the the pre-meal blood glucose, while the larger bandwidth provde long time pattern of the measurement. 

(d) Plot what you think is the “best” bandwidth for each measurement. Comment why you chose this value.


```{r, echo = FALSE, fig.width=10, fig.height = 12}
gp <- ggplot(dat_datetime, aes(x = datetime, y = value, color = code_name)) + 
    geom_point() +
    theme(legend.position='none')

gp1 <- gp +
    geom_smooth(method = 'loess', span = 0.1) +
    labs(title = "Bandwidht = 0.1")
    
gp2 <- gp +
    geom_smooth(method = 'loess', span = 0.2) +
    labs(title = "Bandwidht = 0.2")

gp3 <- gp +
    geom_smooth(method = 'loess', span = 0.5) +
    labs(title = "Bandwidht = 0.5")

gp4 <- gp +
    geom_smooth(method = 'loess', span = 0.9) +
    labs(title = "Bandwidht = 0.9")


grid.arrange(gp1, gp2, gp3, gp4, ncol = 1)
```

In my opinion, there is not much pattern for each measurement when considering longer time period. That is, the values distributed at the similar mean throughout the time. Therefore, I believe that the bandwidth should be at least above 0.5, since the bandwidth below 0.5 might contain unnecessary variations.

(e) Choose another smoothing method discussed in class (i.e. running means, kernel smoothing, splines). Do you think this fits the data better, worse or no different? Justify your conclusions visually.

I choose b-splines for my alternative smoothing method. 
```{r}
gp <- ggplot(dat_datetime, aes(x = datetime, y = value, color = code_name)) + 
    geom_point() +
    theme(legend.position='none')

gp1 <- gp +
    geom_smooth(method = lm, formula = y ~ splines::bs(x, 3), se = TRUE) +
    labs(title = "Smoothing method: B-Spline")

gp2 <- gp +
    geom_smooth(method = 'loess', span = 0.9) +
    labs(title = "Smoothing method: lowess; Bandwidht = 0.9")

grid.arrange(gp1, gp2, ncol = 1)
```

Comparing the spline method and lowess method where the bandwidth = 0.9, the two methods are similar to each other when applying to this dataset.

## Q4. Comparing Results 
Choose someone in the class to compare your results to (who used a different person). Comment on any similarities or differences. Mention the name of the person you worked with.


- The person I compare with: Yiwen
- The patient in my report: #1
- The patient in Yiwen's report: #13
- Comparing results of patient # and #13
    1. results in correlation
        - In patients #1 and #13, the pre-breakfast and pre-lunch both have positive correlation but the correlation in patient #13 is much higher than that in patient #1.
        - In patient #1, the pre-supper measurements is negatively correlated with measurements in pre-breakfast and pre-lunch, while in patient #13, the pre-supper measurements is positively correlated with measurements in other time period.
    2. patterns of pre-meal measurement
        - Based on the smoothing results, the pre-meal blood glucose meausrement have larger variation in the long time period in patient #13 while such pattern was not observed in patient #1.
        
