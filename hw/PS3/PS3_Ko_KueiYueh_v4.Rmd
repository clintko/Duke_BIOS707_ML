---
title: "BIOS707 | Problem set3"
author: "Kuei-Yueh (Clint) Ko"
output:
  html_notebook:
    theme: united
    toc: yes
  pdf_document:
    toc: yes
---

# About the notebook

This the problem set 3 of the course BIOS707. Below are the libraries required for this report.

```{r}
### tools
library(tidyverse)
library(cluster)
library(foreach)
library(clue)
library(pROC)

### plot and display
library(RColorBrewer)
library(pheatmap)
library(corrplot)
library(grid)
library(gridExtra)
```


# Part I. Simulation

## Q1. Explore Clustering
**Simulate a dataset where n = 100 and p = 50. Used your ICC function from problem set 2 to create 3 (k) distinct clusters in dataset.**

```{r}
##### Global variables #####
K          <- 3    # number of group
N          <- 100  # number of observations
SIG2_ALPHA <- 10   # variance of alpha for icc simulation
P          <- 50   # number of parameters
```


```{r}
##### My ICC function #####
simulate_icc <- function(icc, grp_size = 10, n_grp = 5, mu_tot = 0, sig2_a = 1){
    # function to simuate the data from specific ICC
    # =================================
    
    ### calculate sig2_e from the ICC and sig2_a
    sig2_e <- sig2_a * (1 / icc - 1)
    
    ### simulate alpha by sig2_a
    alpha  <- rnorm(n_grp, mean = 0, sd = sig2_a^0.5)
    
    ### simulate error for each group by sig2_e
    df <- sapply(mu_tot + alpha, function(x){
        eps <- rnorm(grp_size, mean = 0, sd = sig2_e^0.5)
        return(x + eps)
    })

    ### set column names
    df <- data.frame(df)
    colnames(df) <- paste("Grp", 1:n_grp)

    ### gather and return
    df <- df %>% gather(Group, Value)
    return(df)
} # end func
```


### Q1 (a) 
**Set your ICC to 0.3 and simulate a dataset. Show your simulation code. Apply K-means clustering and evaluate how well your clusters uncover the true group**

First, I set required function for data simulation and visualization
```{r}
##### helper function in simulate data #####
my_combine <- function(x, y){
    # binary merge function
    # ==========================
    y = y %>% select(-Group)
    return(bind_cols(x, y))
} # end func

##### function that simulate the data #####
simulate_data <- function(icc, k = K, n = N, sig2_a = SIG2_ALPHA, p = P, var_name = "V", func_combine = my_combine){
    # simulate data using the icc simulating function above
    # =======================================
    
    ### simulation using icc function
    dat = replicate(
        p,             # number of parameters
        simulate_icc( # each parameter is simulate independently
            icc, 
            n_grp = k, 
            grp_size = floor(n / k), 
            sig2_a = sig2_a),
        simplify = FALSE)
    
    ### reduced simulated data into one dataframe
    dat = foreach(
         idx = 1:length(dat), 
         .combine = func_combine) %do% {
             df = dat[[idx]]
             df = df %>% dplyr::rename_(
                 .dots=setNames(
                names(.), 
                c("Group", paste0(var_name, idx))))
    } # end do in foreach
    
    return(dat)
} # end func

##### Function to visualize the results #####
plot_dat_sim <- function(dat, variable){
    # visualize simulated data with specify variables
    # ========================================
    
    ### arrange the wide data frame into long format
    df <- dat %>% 
        select(Group, variable) %>% 
        gather(variable, value, -Group)
    
    ### ggplot each variables color by group
    gp <- ggplot(df, aes(x = Group, y = value, color = Group)) + 
        geom_jitter(width = 0.2) +
        theme(
            axis.title.y = element_text(size = 10),
            axis.title.x = element_text(size = 10),
            axis.text.x  = element_text(size = 12, angle = 90, vjust = 0.5),
            strip.text.x = element_text(size = 10, color = "grey30")) +
        facet_wrap(~variable)
    
    return(gp)
} # end func
```

Using the function defined above, I am able to simulate data using any specify ICC and number of parameters. Below is just an example to demonstrate the functions work properly
```{r, echo = FALSE, fig.height = 7, fig.width=5}
### initialization
set.seed(0)

### 
dat_sim = simulate_data(0.9)
gp1 = plot_dat_sim(dat_sim, c("V1", "V10", "V50")) + 
    ggtitle("Simulating random effects with ICC 0.9")

dat_sim = simulate_data(0.5)
gp2 = plot_dat_sim(dat_sim, c("V1", "V10", "V50")) +
    ggtitle("Simulating random effects with ICC 0.5")

grid.arrange(gp1, gp2)
```

Simulate data with ICC 0.3
```{r, fig.height = 3, fig.width = 5}
### initialization
set.seed(0)

### simulation
dat_sim = simulate_data(0.3)

### visualization
gp = plot_dat_sim(dat_sim, c("V1", "V10", "V50")) +
    ggtitle("Simulating random effects with ICC 0.3")
print(gp)
```

The data could also be visualized in a heatmap.
```{r, echo = FALSE, fig.height=7, fig.width=7}
### initialization
set.seed(0)

### simulation
dat_sim = simulate_data(0.3)

### separate group id and values
dat_id  = dat_sim %>% select(Group) 
dat_val = dat_sim %>% select(-Group)

### transpose the matrix and arrange
dat_id  = as.data.frame(dat_id)
dat_val = as.data.frame(t(dat_val))
colnames(dat_val) = rownames(dat_id)

### heatmap and labels
# add heatmap
setHook("grid.newpage", function(){
    pushViewport(
        viewport(
            x=1, y=1, width=0.9, height=0.9, 
            name="vp", just=c("right","top")))}, 
    action="prepend")

pheatmap(dat_val,
    annotation = dat_id,
    show_rownames = FALSE, 
    show_colnames = FALSE)

# add x, y labels
setHook("grid.newpage", NULL, "replace")
grid.text("Observations", y=-0.02, gp = gpar(fontsize=16))
grid.text("Variables (50 Parameters)",    x=-0.07, 
          rot = 90, gp=gpar(fontsize=16))
```


To perform kmean clustering and evaluate how well the clusters uncover the true groups, I implement a function that accept data and number of times to perform kmeans clustering. For each kmeans clustering, a confusion matrix between true group label and cluster id from kmeans is calculated. For the confusion matrix, a Hungarian algorithm is applied to perform cluster alignment. For a perfect clustering case, all the points in one cluster belongs to the same group. This calulation than allows me to get the points that are not cluster correctly, which I define them as **unmatch**.
```{r}
##### function to simulate kmeans clustering with several times #####
simulate_kmean <- function(dat, iteration, is_mat_conf = FALSE){
    ### initialization
    unmatch = c()
    mat = dat %>% select(-Group) %>% as.matrix
    
    ### perform multiple times of kmean clustering
    for (idx in 1:iteration){
        ### get cluster id from kmeans
        clust = kmeans(mat, centers = 3)
    
        ### confusion matrix
        mat_conf = table(clust$cluster, dat_sim$Group)
    
        ### cluster alignment
        idx = solve_LSAP(mat_conf, maximum = TRUE)
        
        ### calculate number of "unmatch" points
        num = sum(mat_conf[cbind(seq_along(idx), idx)])
        unmatch = c(unmatch, nrow(mat) - num) 
    } # end for loop
    
    if (is_mat_conf) {
        ### just to demonstrate what does 
        ### the confusion matrix looks like
        return(list(mat_conf, unmatch))
        
    } else {
        ### normal situation, return the unmatch results
        return(unmatch)
    } # end if-else
    
} # end func
```


Evaluate one kmean clustering using the function defined above
```{r}
### initialization
set.seed(0)

### simulation
dat_sim = simulate_data(0.5)

### kmeans and evaluation
res = simulate_kmean(dat_sim, 1, is_mat_conf = TRUE)
```

from the confution matrix below, I know that the kmean I just ran assgins cluster labels correctly compared to the true label
```{r}
print(res[[1]])
```

Since it is a "perfect" clustering, there is no "unmatch" points
```{r}
cat(res[[2]])
```

Of course, we know that clustering is sometimes sensitive to the intial centers. Therefore, here I define two indices calculated from the variable "unmatch" by running multiple times of kmean clustering

- **expected unmatch event**: if a clustering results has more than 10% unmatch points (in our case, since there are 100 data points, the threshold is 10), this clustering fails to cluster points correctly and an "unmatch event" occur. The number of unmatch event is divided by the number of times the kmean clustering is perform (defined as argument "iteration" in the function simulate_kmean)
- **expected number of unmatch points**: when an unmatch event occur, there might be only few points that are misclustered; therefore, it is important to quantify how well the clustering works by counting the average points that have group label uncovered uncorrectly throughout the multiple kmeans. The average is defined as expected number of unmatch points. Note that the standard deviation of the unmatch points is also calculated to better visualize the variation of the number of unmatch points.

```{r}
#### function to calculate unmatch event and expected number of unmatch points
summary_unmatch <- function(unmatch, threshold = 10, icc = NULL, q = NULL){
    
    ### calculate unmatch event and expected number of unmatch points 
    unmatch_events    = mean(unmatch > threshold)
    unmatch_points_mu = mean(unmatch)
    unmatch_points_sd = sd(unmatch)
    
    ### return the results
    is_exist_icc = !is.null(icc)
    is_exist_q   = !is.null(q)
    
    if (is_exist_icc & is_exist_q){
        res = c(unmatch_events, unmatch_points_mu, unmatch_points_sd, icc, q)
        names(res) = c("events", "points_mu", "points_sd", "icc", "q")
        
    } else if (is_exist_icc) {
        res = c(unmatch_events, unmatch_points_mu, unmatch_points_sd, icc)
        names(res) = c("events", "points_mu", "points_sd", "icc")
        
    } else if (is_exist_q) {
        res = c(unmatch_events, unmatch_points_mu, unmatch_points_sd, q)
        names(res) = c("events", "points_mu", "points_sd", "q")
        
    } else {
        res = c(unmatch_events, unmatch_points_mu, unmatch_points_sd)
        names(res) = c("events", "points_mu", "points_sd")
    }
    
    return(res)
}
```


Here I use one simulated data to demonstrate the idea
```{r}
### initialization
set.seed(0)

### simulate data
dat_sim = simulate_data(0.3)

### clustering
unmatch = simulate_kmean(dat_sim, 100)

### visualization
data.frame(x = 1:length(unmatch), y = unmatch) %>%
    ggplot(., aes(x = x, y = y)) +
    geom_line() +
    geom_point() +
    labs(x = "iteration", y = "number of points mismatch")
```

by the plot above, we can easiy calculate the unmatch events is 9 / 100 = 0.09
```{r}
summary_unmatch(unmatch)
```


### Q1 (b)
**Simulate additional datasets, varying the ICC. Apply K-means clustering with k = 3. Evaluate how well your able to uncover the true cluster labels. Visualize your results and comment on the "required" ICC for effective clustering.**

By defining unmatch events and unmatch points, I am able to evaluate the efficiency of kmean clustering for each datasets generated with different ICC

```{r}
### initialization
iccs = seq(0.1, 1.0, length.out = 10)
ITER_KM = 500
set.seed(0)

### simulation and evaluation
res_km_icc = NULL
for (icc in iccs){
    cat("icc:", icc, "\n")
    dat_sim = simulate_data(icc)
    unmatch = simulate_kmean(dat_sim, ITER_KM)
    unmatch_sum = summary_unmatch(unmatch, icc = icc)
    
    res_km_icc = rbind(res_km_icc, unmatch_sum)
} # end for loop
```

By visualizing the results, we could observe how unmatch events and points varied throughout different kmean clustering. When ICC is 0.1, none of the clustering results is perfect and about 10~15 points are always clustered uncorrectly. As ICC increases from 0.1 to 0.2, probability of unmatch events drop dramatically and once ICC reaches to 1.0, all the clustering results are perfect and therefore, all points have cluster ids and true group labels "matched" correctly.
```{r, echo = FALSE}
plot_unmatch <- function(res_unmatch, iter_km = ITER_KM){
    ###
    df = as.data.frame(res_unmatch)
    
    ###
    gp1 = ggplot(df, aes(x = icc, y = events)) +
        geom_line() + 
        geom_point() +
        labs(x = "ICC", 
             y = "Number of time kmeans \n failed to cluster correctly",
             title = paste("Expected unmathc events (#kmeans:", iter_km, "times)"))
    
    ###
    gp2 = ggplot(df, aes(x = icc, y = points_mu)) +
        geom_ribbon(aes(
            ymin = points_mu - 0.5 * points_sd, 
            ymax = points_mu + 0.5 * points_sd), 
            fill = "grey70") +
        geom_line() + geom_point() +
        labs(x = "ICC", 
             y = "Number of Points uncover \n into wrong groups",
             title = paste("Expected Number of unmatch points", "\n", 
                           "(expected +/- 0.5 * standard deviation)"))
    return(list(gp1, gp2))
    #return(grid.arrange(gp1, gp2, nrow = 2))
}

######################################################
gp = plot_unmatch(res_km_icc)
grid.arrange(gp[[1]], gp[[2]], nrow = 2)
```

### Q1 (c)
**Choose the minimum ICC from above where you think you can uncover the clusters. Keep n and p the same but vary the number of variables that have an ICC. i.e. some q < p will have an ICC where the rest will have an ICC of 0. These variables with ICC = 0 are called noise." Assess the impact of the amount of noise variables on ability to recapture clusters.**


First, I set the function to add "noisy" parameters in data.
```{r}
simulate_data_noise <- function(icc, q, k = K, n = N, sig2_a = SIG2_ALPHA, p = P, func_sim_data = simulate_data){
    
    ### assert if q is larger than p
    if (q > p){
        cat("ERROR: argument q > p")
    }
    
    ### special case
    if (q == 0){
        dat = simulate_data(0.00001)
        
    } else if (q == 50) {
        dat = simulate_data(icc)
        
    } else {
    ### normal case where q != p
        n1 = q
        n2 = p-q
    
        ### simulate two data: one with specified icc and another one with extreme small icc
        dat1 = func_sim_data(icc,    k, n, sig2_a, n1, var_name = "V")
        dat2 = func_sim_data(0.00001, k, n, sig2_a, n2, var_name = "N")
        
        ### combine the results
        tmp = dat2 %>% select(-Group)
        dat = bind_cols(dat1, tmp)
    } # end if else
    
    return(dat)
} # end func
```


Check if the simulation works properly
```{r}
### initialization
set.seed(0)

### simulation
dat_sim = simulate_data_noise(0.1, 10)

### visualization
gp = plot_dat_sim(dat_sim, c("V1", "V10", "N1", "N2"))
print(gp)
```

The results of the impact of noise is showed together with the simulation in the next question.

### Q1 (d)
**Perform the two analyses above across a full grid of ICCs and noise.Evaluate your results. What patterns do you see? Is there an interaction between noise and ICC?**

simulation with different ICC and noise
```{r}
### intialization
iccs = seq(0.1, 1.0, length.out = 10)
qs   = c(0, 1, 25, 49, 50)
ITER_KM = 500
set.seed(0)

### simulation and collect the results
res_km_icc_q = NULL
for (icc in iccs){
    cat("\nicc:", icc)
    
    for (q in qs){
        cat("#")
        
        dat_sim = simulate_data_noise(icc, q)
        
        unmatch = simulate_kmean(dat_sim, ITER_KM)
        unmatch_sum = summary_unmatch(unmatch, icc = icc, q = q)
        
        res_km_icc_q = rbind(res_km_icc_q, unmatch_sum)
    } # end for loop
} # end for loop
```


Visualize the results: when q = 50, the pattern is exactly the same as we show above, where no parameters has ICC ~ 0. However, from the number of unmatched events, I supprisingly observed that once a pure "noisy" parameters (parameter with ICC ~ 0) is added, the kmeans clustering performs very unefficiently. None of the kmeans clustering results is "perfect".
```{r, echo = FALSE}
plot_unmatch_q <- function(res_unmatch_q, iter_km = ITER_KM){
    #
    
    ###
    df = as.data.frame(res_unmatch_q)
    
    gp1 = ggplot(df, aes(x = as.character(q), 
                        y = as.character(icc), 
                        fill = events)) + 
        geom_tile() + 
        scale_fill_distiller(
            palette = "Reds", 
            direction = 1) +
        labs(x = "q", y = "ICC",
             title = "Number of time kmeans \n failed to cluster correctly")
    
    gp2 = ggplot(df, aes(x = as.character(q), 
                        y = as.character(icc), 
                        fill = points_mu)) + 
        geom_tile() + 
        scale_fill_distiller(
            palette = "Reds", 
            direction = 1) +
        labs(x = "q", y = "ICC",
             title = "Expected points uncover \n into wrong groups")

    return(list(gp1, gp2))
    #return(grid.arrange(gp1, gp2, nrow = 2))
}

gp = plot_unmatch_q(res_km_icc_q)
grid.arrange(gp[[1]], gp[[2]], nrow = 2)
```

## Q2 PCA and Clustering
**While PCA solves a different problem than clustering it is also used to find structure in data. **

Note that in this section I have implemented two helper functions plot_pca_scree and plot_pca_score to visualize the results
```{r, echo = FALSE}
#### helper function to visualize the PCA results
plot_pca_scree <- function(res_pca){
    ###
    x = res_pca$sdev
    x = x^2
    x = x / sum(x)
  
    ###
    df = data.frame(
        PC   = 1:length(x),
        prop = x, 
        cump = cumsum(x))

    idx = which(df$cump > 0.9)[1]

    ###   
    gp1 = ggplot(df, aes(x = PC, y = prop))
    gp1 = gp1 +
       geom_point() +
       geom_linerange(aes(ymax = prop, ymin = 0)) +
       labs(x = "Number of PC",
            y = "Proportion of\nexplained variance")

    gp2 = ggplot(df, aes(x = PC, y = cump))    
    gp2 = gp2 +
        geom_point() +
        geom_linerange(aes(ymax = cump, ymin = 0)) +
        labs(title = paste("#PC to explained 90% of variance:", idx),
             x = "Number of PC",
             y = "Cumulative proportion of\nexplained variance")
    
    return(list(gp1, gp2))
} # end func


#### helper function to visualize the PCA results
plot_pca_score <- function(res_pca, vec_for_color = NULL){
    
    x = res_pca$sdev
    x = x^2
    x = x / sum(x)
    
    if (is.null(vec_for_color)){
        gp = data.frame(
            pc1 = res_pca$scores[, 1],
            pc2 = res_pca$scores[, 2]) %>%
            ggplot(., aes(x = pc1, y = pc2))
        
    } else {
        gp = data.frame(
            pc1 = res_pca$scores[, 1],
            pc2 = res_pca$scores[, 2],
            x_col = vec_for_color) %>%
            ggplot(., aes(x = pc1, y = pc2, color = x_col))
    } # end if-else
    
    gp = gp +
        geom_point() +
        labs(x = paste("PC1:", round(x[1], 3) * 100, "%"),
             y = paste("PC2:", round(x[2], 3) * 100, "%"),
            title = paste("Total variation explained:", 
                          round(x[1] + x[2], 3) * 100,
                          "%"))

    return(gp)
} # end func
```


### Q2 (a)
**Choose a dataset from 1b where you were able to uncover clusters. Apply PCA. How many meaningful PCs do you find? Plot PC1 vs PC2.**


Here I used ICC = 0.3, and the result shows that there are about two meaningful PCs.
```{r}
### initalization
set.seed(0)

### simulation
dat_sim = simulate_data(0.3)

### PCA analysis
dat     = dat_sim %>% select(-Group)
res_pca = princomp(dat)

### visualization
gp = plot_pca_scree(res_pca)
grid.arrange(gp[[1]] + ggtitle("ICC: 0.3"), 
             gp[[2]])
```

The number PCs required to explain 90% of total variance decrease when ICC increase
```{r}
### initalization
set.seed(0)

### simulation
dat_sim = simulate_data(0.9)

### PCA analysis
dat     = dat_sim %>% select(-Group)
res_pca = princomp(dat)

### visualization
gp = plot_pca_scree(res_pca)
grid.arrange(gp[[1]] + ggtitle("ICC: 0.9"), 
             gp[[2]])
```

Using ICC = 0.3, three groups are easily observed in the plot
```{r, fig.height = 4, fig.width = 5}
### initialization
set.seed(0)

### simulation
dat_sim = simulate_data(0.3)

### get the true label
grp     = dat_sim %>% select(Group)
grp     = grp$Group

### get the simulated values
dat     = dat_sim %>% select(-Group)

### perform PCA and visualize
res_pca = princomp(dat)
plot_pca_score(res_pca, grp)
```

As ICC increase, three groups are becomes more and more apparent in the plot
```{r, fig.height = 4, fig.width = 5}
### initialization
set.seed(0)

### simulation
dat_sim = simulate_data(0.9)

### get the true label
grp     = dat_sim %>% select(Group)
grp     = grp$Group

### get the simulated values
dat     = dat_sim %>% select(-Group)

### perform PCA and visualize
res_pca = princomp(dat)
plot_pca_score(res_pca, grp)
```

### Q2 (b)
**Repeat but set the number of groups k to be 4. What relationships do you notice between the number of "true groups" and the number of meaningful components.**

When k becomes 4, it is impossible to distinguish two groups in the PCA 2D plot.
```{r, fig.height = 4, fig.width = 5}
### intialization
set.seed(0)

### simulation
dat_sim = simulate_data(0.3, k = 4)

### get the group label
grp     = dat_sim %>% select(Group)
grp     = grp$Group

### get the simulated values
dat     = dat_sim %>% select(-Group)

### PCA and visualize
res_pca = princomp(dat)
plot_pca_score(res_pca, grp)
```

The problem kind of remains even when I raises the ICC to 0.9, even though the situation is much better than the case when ICC is 0.3.
```{r, fig.height = 4, fig.width = 5}
### intialization
set.seed(0)

### simulation
dat_sim = simulate_data(0.9, k = 4)

### get the group label
grp     = dat_sim %>% select(Group)
grp     = grp$Group

### get the simulated values
dat     = dat_sim %>% select(-Group)

### PCA and visualize
res_pca = princomp(dat)
plot_pca_score(res_pca, grp)
```

# Part II. Working with Data
**Download the Mice Protein Expression data set from the UCI Machine Learning
Repository: https://archive.ics.uci.edu/ml/datasets/Mice+Protein+Expression**

**This dataset contains 77 protein expression values for 1080**


I have downloaded the data and converted it into csv file to read into R.
```{r}
dat_mice = read_csv("./Data_Cortex_Nuclear.csv")
head(dat_mice)
```


## Q1. Exploring / Clearing Data

### Q1 (a)
**Focusing just on the 77 protein measurements provide any relevant descriptive statistics and/or visualizations.**

First, separate phenodata from the expression matrix.
```{r}
idx = c("MouseID", "Genotype", "Treatment", "Behavior", "class")
dat_pheno = dat_mice %>% select(idx)
dat_exprs = dat_mice %>% select(-idx)
```

Visualize the 77 protein measurements. Since some protein expression much higher than others, each protein measurement is standardized.
```{r, echo = FALSE, fig.height=7, fig.width=7}
### transpose the matrix and arrange
dat_id  = as.data.frame(dat_pheno)
dat_val = as.data.frame(t(dat_exprs))
colnames(dat_val) = rownames(dat_id)

### heatmap and labels
# add heatmap
setHook("grid.newpage", function(){
    pushViewport(
        viewport(
            x=1, y=1, width=0.9, height=0.9, 
            name="vp", just=c("right","top")))}, 
    action="prepend")

pheatmap(dat_val,
    scale = "row",
    annotation = dat_id,
    show_rownames = FALSE, 
    show_colnames = FALSE)

# add x, y labels
setHook("grid.newpage", NULL, "replace")
grid.text("Observations (Mouse)", y=-0.02, gp = gpar(fontsize=16))
grid.text("Variables (Protein)",    x=-0.07, 
          rot = 90, gp=gpar(fontsize=16))
```

### Q1 (b) 
**Remove NA > 10% and impuate the remained NA values using single mean imputation.**

```{r}
single_mean_impute = function(dat){
    dat_imputed = lapply(dat, function(x){
        mu = mean(x, na.rm = TRUE)
        x[is.na(x)] = mu
        return(x)
    })
    return(do.call(cbind, dat_imputed))
} # end func

dat_exprs_imp = single_mean_impute(dat_exprs) %>% as.tibble
```

## Q2. PCA
### Q2 (a)

About two meaningful PCs are observed.
```{r}
res_pca = princomp(dat_exprs_imp)

### visualization
gp = plot_pca_scree(res_pca)
grid.arrange(gp[[1]], gp[[2]])
```

### Q2 (b)
**Scale the data and repeat. Comment on any differences.**

Two meaningful PCs are picked after scaling the dataset. Comparing to unscaled data, more PCs are required to explain at least 90% of total vairation.
```{r}
dat_exprs_scale = scale(dat_exprs_imp)
res_pca = princomp(dat_exprs_scale)

### visualization
gp = plot_pca_scree(res_pca)
grid.arrange(gp[[1]], gp[[2]])
```

### Q2 (c)
**Columns 79 - 82 provide labels to the data. Using the unscaled data to visualize the first couple of PCs and see whether they are able to separate out any of the groups.**

From the plot, non of labels are able to be distinguished by the first two PCs. Most of the labels seems to have no relationship with the first two PCs except "Behaviors". In the label "Behaviors", a lot of data points in "S/C" group has less PC2 value than the data points in "C/S" group, even though pc2 itself is not able to seperate these two groups.
```{r}
res_pca = princomp(dat_exprs_imp)

df = dat_pheno
df$pc1 = res_pca$scores[, 1]
df$pc2 = res_pca$scores[, 2]


my_geom = geom_point(size = 0.5)

gp1 = ggplot(df, aes(x = pc1, y = pc2, color = Genotype)) + 
    my_geom +
    labs(title = "Genotype")

gp2 = ggplot(df, aes(x = pc1, y = pc2, color = Treatment)) + 
    my_geom +
    labs(title = "Treatment")

gp3 = ggplot(df, aes(x = pc1, y = pc2, color = Behavior)) + 
    my_geom +
    labs(title = "Behavior")

gp4 = ggplot(df, aes(x = pc1, y = pc2, color = class)) + 
    my_geom +
    labs(title = "class")

grid.arrange(gp1, gp2, gp3, gp4, nrow = 2, ncol = 2)
```


### Q2 (d) 
**The PCs can be used in a regression model**

Here I have implemented a helper function called **concordance** to test the performance of a fitted logistic model
```{r, echo = FALSE}
concordance <- function(model){
    
    # Get all actual observations and their fitted values into a frame
    fitted <- data.frame(cbind(model$y, model$fitted.values))
    colnames(fitted)<-c('respvar','score')
    
    # Subset only ones
    ones  <- fitted[fitted[,1] == 1,]
    # Subset only zeros
    zeros <- fitted[fitted[,1] == 0,]
    
    #print(ones)
    #print(zeros)
    # Initialise all the values
    pairs_tested <- 0
    conc         <- 0
    disc         <- 0
    ties         <- 0
      
    # Get the values in a for-loop
    for(i in 1:nrow(ones)) {
          
        for(j in 1:nrow(zeros)) {
            pairs_tested<-pairs_tested + 1
            
            if        (ones[i, 2] >  zeros[j, 2]) { 
                conc <- conc + 1 
            } else if (ones[i, 2] == zeros[j, 2]){ 
                ties <- ties + 1 
            } else { 
                disc <- disc + 1 
            } # end if-else
        } # end inner for
    } # end outer for
    
    # Calculate concordance, discordance and ties
    concordance <- conc / pairs_tested
    discordance <- disc / pairs_tested
    ties_perc   <- ties / pairs_tested
    n           <- nrow(fitted)
    
    # index
    # http://support.sas.com/kb/45/767.html
    # http://support.sas.com/documentation/cdl/en/statug/66103/HTML/default/viewer.htm#statug_surveylogistic_details64.htm
    return(
        list(
            "Concordance" = concordance,
            "Discordance" = discordance,
            "Tied"        = ties_perc,
            "Pairs"       = pairs_tested,
            
            ### Somers' D
            "Somers D"    = (concordance - discordance),
            
            ### Goodman-Kruskal Gamma
            "gamma"       = (concordance - discordance) / (concordance + discordance),
            
            ### Kendall's Tau-a
            "tau-a"       = (conc - disc) / (0.5 * n * (n - 1)),
            
            ### c-statistics
            "c"           = auc(model$y, model$fitted.values)
        ) # end list
    ) # end return
} # end func
```


#### Q2 (a) (i)

using original expression value
```{r}
df = dat_exprs_imp
df$Genotype = dat_pheno$Genotype
df$Genotype = factor(df$Genotype, levels = c("Control", "Ts65Dn"))
df$Genotype = as.numeric(df$Genotype) - 1

fit_logit_ori = glm(Genotype ~ ., data = df, family = binomial(link = "logit"))
```

Evaluate the performance of the model. Since there are two many parameters fitted, I believe the model is overfit based on the statisics calculated below.
```{r}
concordance(fit_logit_ori)
```


#### Q2 (a) (ii)

Regress the Genotype variable onto the first few "meaningful" PCs from 2a
```{r}
df = dat_pheno
df$pc1 = res_pca$scores[, 1]
df$pc2 = res_pca$scores[, 2]
#df$pc3 = res_pca$scores[, 3]
#df$pc4 = res_pca$scores[, 4]

df$Genotype = factor(df$Genotype, levels = c("Control", "Ts65Dn"))
df$Genotype = as.numeric(df$Genotype) - 1

fit_logit_pc2 = glm(Genotype ~ pc1 + pc2, data = df, family = binomial(link = "logit"))
summary(fit_logit_pc2)
```

Evaluate the model contructed using the first two meaningful PCs. The model performs badly and this could be supported from the PCA plot before, where the first two PCs is not able to distinguish different groups in the label "Genotype"
```{r}
concordance(fit_logit_pc2)
```


## Q3. Clustering

### Q3 (a) K-Mediods

#### Q3 (a) (i) Optimal K

By performing K-Mediods clustering using different k, I believe there are about 2~5 groups. The optimal k indicated by the Silhouette score is k=3.
```{r}
### intialization 
set.seed(0)
sil_score = rep(0, 10)

### clustering under different k
for (k in 2:5){
    clust = pam(dat_exprs_imp, k = k)
    sil   = silhouette(clust)
    sil_score[k] = mean(sil[, 3])    
} # end for loop

### visualization
qplot(1:length(sil_score), sil_score, geom = c("line", "point")) +
    scale_x_continuous(breaks = 1:length(sil_score))
```


#### Q3 (a) (ii) cluster groups vs labels

By performing k-mediods with k = 3, we can then visualize the count observations of each labels in different clusters. However, I do not observe a clear pattern in each table. For example, the "class" label, even though it seems that t-SC-m, c-SC-m and t-SC-s group mostly exist in cluster 3, this does not 100% imply that the k-mediod clusters performed are able to group each "class" labels into different clusters.
```{r, echo = FALSE}
### intialization 
set.seed(0)

### perform clustering using k = 3
clust_mediods = pam(dat_exprs_imp, k = 3)
df  = dat_pheno %>% select(-MouseID)

### compare cluster to each label
res = lapply(df, function(x){
    df = table(clust_mediods$clustering, x) %>% as.matrix
    rownames(df) = paste("cluster", 1:3)
    return(df)
}) # end lapply

### visualize the table
for (mat in res){
    pheatmap(
        mat,
        #color = colorRampPalette(rev(brewer.pal(n = 7, name = "RdYlBu")))(100))
        color = colorRampPalette(brewer.pal(n = 7, name = "Reds"))(100),
        treeheight_row = 0,
        treeheight_col = 0,
    ) # end pheatmap
} # end for loop
```


#### Q3 (a) (iii) cluster protein

Using the same method above, I first find possible k values to cluster protein. Since I am going to plot the correlation matrix and the cluster id is just to order the protein in the correlation matrix, I choose the largest possible k which is 5
```{r, echo = FALSE}
sil_score = rep(0, 10)

for (k in 2:5){
    clust = pam(t(dat_exprs_imp), k = k)
    sil   = silhouette(clust)
    sil_score[k] = mean(sil[, 3])    
} # end for loop

qplot(1:length(sil_score), sil_score, geom = c("line", "point")) +
    scale_x_continuous(breaks = 1:length(sil_score))
```

plot the correlation matrix
```{r}
### cluster to order the protein
clust_mediods_protein = pam(t(dat_exprs_imp), k = 5)
idx = order(clust_mediods_protein$cluster)
print(idx)

### set colors
colors = brewer.pal(name = "RdYlBu", n = 8)
colors = rev(colors)
colors = colorRampPalette(colors)(100)

### plot correlation matrix
res = cor(dat_exprs_imp[,idx])
corrplot(res, col = colors, tl.cex=0.2)
```


### Q3 (b) Hierarchical Clustering

#### Q3 (b) (i) Clustering Method

apply different methods and compare the size of each clusters under different methods. It turns out that the size of each cluster is similar in the complete method, while in single and average linkage, most of the data points are within a clusters.
```{r}
### convert correlation matrix to dissimilar matrix
mat = cor(t(dat_exprs_imp))
mat = 1 - mat
mat = as.dist(mat)

methods = c("single", "complete", "average")
res = lapply(methods, function(method){
    tmp = hclust(mat, method = method)
    tmp = table(cutree(tmp, k = 4))
    return(tmp)
})

res = do.call(rbind, res)
rownames(res) = methods
res
```

#### Q3 (b) (ii) Standardizing Data

I decided to repeat the procedure above but now using the scaled data, where each protein measurement is standardized. Interestingly, When clustering on scaled data, the complete and average now has similar sizes of all clusters, while under single linkage, most of the points are still in one clusters, similar to the results of unscaled data.
```{r}
### convert correlation matrix to dissimilar matrix
mat = cor(t(dat_exprs_scale))
mat = 1 - mat
mat = as.dist(mat)

methods = c("single", "complete", "average")
res = lapply(methods, function(method){
    tmp = hclust(mat, method = method)
    tmp = table(cutree(tmp, k = 4))
    return(tmp)
})

res = do.call(rbind, res)
rownames(res) = methods
res
```


#### Q3 (b) (iii)  Distance Metric

Using the distance matrix from standandized data, the results is similar to the correlation amtrix in unstandardized data
```{r}
mat = dist(dat_exprs_scale)

methods = c("single", "complete", "average")
res = lapply(methods, function(method){
    tmp = hclust(mat, method = method)
    tmp = table(cutree(tmp, k = 4))
    return(tmp)
})

res = do.call(rbind, res)
rownames(res) = methods
res
```


It turns out that no matter the data is scaled or not, the distribution of cluster size is similar to the results with correlation matrix and unstandardized data.
```{r}
mat = dist(dat_exprs_imp)

methods = c("single", "complete", "average")
res = lapply(methods, function(method){
    tmp = hclust(mat, method = method)
    tmp = table(cutree(tmp, k = 4))
    return(tmp)
})

res = do.call(rbind, res)
rownames(res) = methods
res
```

#### Q3 (b) (iv) Sampling

I sampling 20% of the data (observations) and compare cluster membership. Here I decide to use silhouette scores to indicate the stability of clustering. I will perform the clustering multiple times and compare the Silhouette scores. My idea is that if the clustering is stable across different intializaiton of centers and methods, the Silhouette scores should be similar no matter which intial centers and linkage methods are chosen. Here I show the results of using distance matrix in one cluster results, but I will perform multiple times of clustering using both distance and correlation matrix under different linkage methods in the next question.
```{r}
### initialization
mat = dist(dat_exprs_scale)
set.seed(0)
methods = c("single", "complete", "average")

### clustering
res = lapply(methods, function(method){
    tmp = hclust(mat, method = method)
    tmp = cutree(tmp, k = 4)
    
    sil = silhouette(tmp, mat)
    sil = mean(sil[, 3])
    return(sil)
})

### silhouette scores under different linkage
names(res) = methods
res= unlist(res)
res
```


#### Q3 (b) (v) Comment

```{r, echo = FALSE}
set.seed(0)

ITER_HC = 100
methods = c("single", "complete", "average")

dat = dat_exprs_scale
res_hc_sampling = NULL


for (dummy_num in 1:ITER_HC){
    df = dat %>% as.data.frame %>% sample_frac(0.2)
    mat = dist(df)
    
    
    res = lapply(methods, function(method){
        tmp = hclust(mat, method = method)
        tmp = cutree(tmp, k = 4)
    
        sil = silhouette(tmp, mat)
        sil = mean(sil[, 3])
        return(sil)
    }) # end lapply
    
    names(res) = methods
    res= unlist(res)
    
    res_hc_sampling = rbind(res_hc_sampling, res)
} # end for loop

#res_hc_sampling
dat = res_hc_sampling %>% 
    as.data.frame %>% 
    mutate(iteration = 1:ITER_HC) %>%
    gather(method, score, -iteration)

gp = ggplot(dat, aes(
    x = iteration, 
    y = score, group = method, 
    color = method)) + 
    geom_line() + 
    labs(y = "silhouette score", title = "Scaled data ; Sampling 20% ; distance matrix")

gp_scale_dist = gp

#####################################################

#set.seed(0)

ITER_HC = 100
methods = c("single", "complete", "average")

dat = dat_exprs_imp
res_hc_sampling = NULL
for (dummy_num in 1:ITER_HC){
    
    df = dat %>% as.data.frame %>% sample_frac(0.2)
    mat = dist(df)
    
    
    res = lapply(methods, function(method){
        tmp = hclust(mat, method = method)
        tmp = cutree(tmp, k = 4)
    
        sil = silhouette(tmp, mat)
        sil = mean(sil[, 3])
        return(sil)
    }) # end lapply
    
    names(res) = methods
    res= unlist(res)
    
    res_hc_sampling = rbind(res_hc_sampling, res)
} # end for loop

#res_hc_sampling
dat = res_hc_sampling %>% 
    as.data.frame %>% 
    mutate(iteration = 1:ITER_HC) %>%
    gather(method, score, -iteration)

gp = ggplot(dat, aes(
    x = iteration, 
    y = score, group = method, 
    color = method)) + 
    geom_line() + 
    labs(y = "silhouette score", title = "Unscaled data ; Sampling 20% ; distance matrix")

gp_imp_dist = gp

#####################################################

#set.seed(0)

ITER_HC = 100
methods = c("single", "complete", "average")

dat = dat_exprs_scale
res_hc_sampling = NULL

for (dummy_num in 1:ITER_HC){
    df = dat %>% as.data.frame %>% sample_frac(0.2)
    mat = cor(t(df))
    mat = 1 - mat
    mat = as.dist(mat)
    
    res = lapply(methods, function(method){
        tmp = hclust(mat, method = method)
        tmp = cutree(tmp, k = 4)
    
        sil = silhouette(tmp, mat)
        sil = mean(sil[, 3])
        return(sil)
    }) # end lapply
    
    names(res) = methods
    res= unlist(res)
    
    res_hc_sampling = rbind(res_hc_sampling, res)
} # end for loop

#res_hc_sampling
dat = res_hc_sampling %>% 
    as.data.frame %>% 
    mutate(iteration = 1:ITER_HC) %>%
    gather(method, score, -iteration)

gp = ggplot(dat, aes(
    x = iteration, 
    y = score, group = method, 
    color = method)) + 
    geom_line() + 
    labs(y = "silhouette score", title = "Scaled Data; Sampling 20% ; Correlation")

gp_scale_cor = gp

#####################################################

#set.seed(0)

ITER_HC = 100
methods = c("single", "complete", "average")

dat = dat_exprs_imp
res_hc_sampling = NULL

for (dummy_num in 1:ITER_HC){
    df = dat %>% as.data.frame %>% sample_frac(0.2)
    mat = cor(t(df))
    mat = 1 - mat
    mat = as.dist(mat)
    
    res = lapply(methods, function(method){
        tmp = hclust(mat, method = method)
        tmp = cutree(tmp, k = 4)
    
        sil = silhouette(tmp, mat)
        sil = mean(sil[, 3])
        return(sil)
    }) # end lapply
    
    names(res) = methods
    res= unlist(res)
    
    res_hc_sampling = rbind(res_hc_sampling, res)
} # end for loop

#res_hc_sampling
dat = res_hc_sampling %>% 
    as.data.frame %>% 
    mutate(iteration = 1:ITER_HC) %>%
    gather(method, score, -iteration)

gp = ggplot(dat, aes(
    x = iteration, 
    y = score, group = method, 
    color = method)) + 
    geom_line() + 
    labs(y = "silhouette score", title = "Unscaled Data; Sampling 20% ; Correlation")

gp_imp_cor = gp

##################################################

my_geom = theme(title = element_text(size = 7))
grid.arrange(gp_imp_dist + my_geom, 
             gp_imp_cor + my_geom, 
             gp_scale_dist + my_geom, 
             gp_scale_cor + my_geom)
```

The figures above shows that when the data is not scaled, the Silhouette scores distributed in a wider range and the range does not change a lot under different methods (excepts that the variation of scores under single linkage is higher than others).

However, when the data is standardized (scaled), the distribution of the Silhouette scores are different under different methods and matrix chosen. For example, it is clearly showed that the distribution of scores are similar for average and complete linkage using correlation matrix to perform hierarchical clustering.

Note that once the data is standardized, the variation of the scores is smaller than teh unstandardized one. Therefore, I believe no matter which linkage methods or matrices are chosen, the standardized data yield more stable clustering results.





